Cloud Learning
Hadoop
Sudo apt update
Sudo apt install ssh
Wget hadoop link
Tar -xzf hadoop file
Sudo apt install openjdk-11-jdk
Mv hadoop-3.3.6 hadoop
Cd hadoop
Mkdir data
Cd data
Mkdir -p {dataode,namenode}
Come back
Gedit ~/.bashrc
Source ~/.bashrc
Cd /etc/hadoop
Java location- dirname $(dirname $(readlink -f $(which java)))
Gedit hadoop-env.sh
Gedit core-site.xml
Gedit hdfs-site.xml
Gedit yarn-site.xml
Gedit mapred-site.xml
Ssh-keygen -t rsa
Cat ~/.ssh/id_rsa.pub>>~/.ssh/authorized_keys
Hdfs -chmod 640 ~/.ssh/authorized_keys
start-hdfs.sh
start-yarn.sh
Hdfs namenode -format
Jps
Create count.txt in hadoop
Hdfs -mkdir /user/jagath
Hdfs -put /home/jagath/hadoop/count.txt /user/jagath/
Use ls to check
Hdfs -chmod 755 /user/jagath/count.txt
Hadoop jar /home/jagath/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /user/jagath/count.txt /output
Hadoop fs -ls /output
Take the file name and use cat cmd
Hadoop
1
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/home/jagath/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

2
# Hadoop Configuration
# For core-site.xml
<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>


3
#For hdfs-site.xml or https-site.xml
<property>
<name>dfs.replication</name>
<value>3</value>
/property><property> <
<name>dfs.name.dir</name>
<value>/home/jagath/hadoop/data/namenode</value>
</property><property>
<name>dfs.data.dir</name>
<value>/home/jagath/hadoop/data/datanode</value>
</property>

4
# For yarn-site.xml
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property><property>
<name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

5
# For mapred-site.xml
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${/home/jagath/hadoop/share/hadoop/mapreduce}</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${/home/jagath/hadoop/share/hadoop/mapreduce}</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${/home/jagath/hadoop/share/hadoop/mapreduce}</value>
</property>

Spark
Sudo apt install openjdk-11-jdk
Sudo apt install scala
Wget spark
Tar -xzf spark_file_name
Mv spark_file_name /usr/local/spark
Gedit ~/.bashrc
Export PATH=$PATH:/usr/local/spark/bin
Spark-scala
Ls /usr/local/spark/examples/jars
./bin/spark-submit --class org.apache.spark.examples.LocalPi -master local[4] /usr/local/spark/examples/jars/………….. 100
MPI
Sudo apt install python3
Sudo apt install -y -qq mpich
Sudo apt install python3-pip
Sudo apt install python3-mpi4py
Mpirun -np 4 python3 mpi_sample.py
from mpi4py import MPI
import numpy as np

comm=MPI.COMM_WORLD
rank=comm.Get_rank()
size=comm.Get_size()

if rank==0:
	data=np.arange(100,dtype="i")
	chunks=np.array_split(data,size)
else:
	chunks=None
chunk=comm.scatter(chunks,root=0)
local_sum=np.sum(chunk);
total_sum=comm.gather(local_sum,root=0)
if rank==0:
	final_sum=np.sum(total_sum)
	print("The final sum is ",final_sum)


#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

int main()
{
    int total=0;
    #pragma omp parallel num_threads(4)
    {
        int i;
        int local=0;
        for(int i=0;i<100;i++)
        {
            local+=i;
        }
        int id=omp_get_thread_num();
        #pragma omp critical
        {
            printf("The sum %d calculated by thread%d\n",local,id);
            total+=local;
        }
    }
    printf("The total sum is %d",total);
    return 0;
}

#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

int main()
{
    int total_points = 1000000; // Total random points
    int inside_circle = 0;      // Points inside the circle
    double pi;

    #pragma omp parallel for reduction(+:inside_circle)
    for (int i = 0; i < total_points; i++)
    {
        double x = (double)rand() / RAND_MAX;
        double y = (double)rand() / RAND_MAX;

        if (x * x + y * y <= 1.0)
        {
            inside_circle++;
        }
    }

    pi = 4.0 * inside_circle / total_points;
    printf("Estimated value of Pi: %f\n", pi);
    return 0;
}




FROM python:3.9-slim
WORKDIR /app
COPY simple.py /app/
CMD ["python","simple.py"]

def fibonacci(n):
    fib_sequence = [0, 1]
    for i in range(2, n):
        next_number = fib_sequence[i-1] + fib_sequence[i-2]
        fib_sequence.append(next_number)
    return fib_sequence

# Example usage
n_terms = 10  # Change this to the number of terms you want
print(fibonacci(n_terms))


